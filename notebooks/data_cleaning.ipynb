{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focal-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "passing-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import datetime\n",
    "from functools import partial\n",
    "from dateutil import parser, tz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-banner",
   "metadata": {},
   "source": [
    "# Stanford Sentiment Treebank v2\n",
    "\n",
    "https://www.kaggle.com/atulanandjha/stanford-sentiment-treebank-v2-sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"../data/sst2/dictionary.txt\", sep='|', header=None)\n",
    "text.columns = ['text', 'id']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "text['text'].apply(len).plot.hist(bins=30)\n",
    "plt.xlabel('Text Length (characters)')\n",
    "print(text.info())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../data/sst2/sentiment_labels.txt\", sep='|')\n",
    "labels.columns = ['id', 'sentiment']\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['sentiment'].plot.hist(bins=30)\n",
    "plt.xlabel('Sentiment (0-1)')\n",
    "print(labels.info())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2 = text.merge(labels, how='inner', on='id')\n",
    "print(sst2.info())\n",
    "sst2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "experiment = sst2.copy()\n",
    "experiment['len'] = experiment['text'].str.len()\n",
    "experiment = experiment.groupby('len')\n",
    "\n",
    "text_reg = experiment['sentiment'].mean()\n",
    "text_count = experiment.apply(len)\n",
    "\n",
    "sns.regplot(x=text_reg.index, y=text_reg, ax=ax1)\n",
    "ax1.set_title('Linear Regression of Text Length against Sentiment')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "ax2.plot(text_count.index, text_count)\n",
    "ax2.set_title('Frequency of samples at each length')\n",
    "ax2.set_xlabel('Text Length')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "del experiment, text_reg, text_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 11):\n",
    "    sst2[f\"{i}lab\"] = pd.cut(sst2['sentiment'], i).cat.codes\n",
    "sst2.set_index('id', inplace=True)\n",
    "sst2.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2.to_csv('../data/sst2/sst2_2_10.csv')\n",
    "del sst2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-dallas",
   "metadata": {},
   "source": [
    "# Sentiment140\n",
    "\n",
    "https://www.kaggle.com/https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140 = pd.read_csv('../data/sentiment140/sentiment140.csv', encoding='latin-1', header=None, usecols=[0, 1, 2, 4, 5])\n",
    "sentiment140.columns = ['sentiment', 'id', 'date', 'author', 'text']\n",
    "\n",
    "date_parser = partial(parser.parse, tzinfos={'PDT': tz.gettz('America/Los Angeles')})\n",
    "sentiment140['date'] = sentiment140['date'].progress_apply(date_parser)\n",
    "sentiment140['sentiment'] = sentiment140['sentiment'].astype(int).values >> 2  # Convert [0, 4] label to [0, 1]\n",
    "sentiment140.set_index('id', inplace=True)\n",
    "sentiment140.sort_index(inplace=True)\n",
    "print(sentiment140.info())\n",
    "sentiment140.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "experiment = sentiment140.copy()\n",
    "experiment['day'] = experiment['date'].dt.date\n",
    "experiment['len'] = experiment['text'].str.len()\n",
    "grouped = experiment.groupby('day')\n",
    "\n",
    "experiment['len'].plot.hist(bins=30, ax=ax1)\n",
    "grouped.apply(len).plot(ax=ax2)\n",
    "grouped['sentiment'].mean().plot(ax=ax4)\n",
    "text_reg = experiment.groupby('len')['sentiment'].mean()\n",
    "sns.regplot(x=text_reg.index, y=text_reg, ax=ax3)\n",
    "\n",
    "ax1.set_title('Histogram of Text Lengths')\n",
    "ax1.set_xlabel('Text Length')\n",
    "ax2.set_title('Number of Tweets per Day')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax3.set_title('Linear Regression of Text Length against Sentiment')\n",
    "ax3.set_xlabel('Text Length')\n",
    "ax3.set_ylabel('Sentiment')\n",
    "ax4.set_title('Average Sentiment per Day')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Sentiment')\n",
    "plt.subplots_adjust(top=1.2, right=1.5)\n",
    "plt.show()\n",
    "\n",
    "del experiment, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140.to_csv('../data/sentiment140/sentiment140_binary.csv', encoding='latin-1')\n",
    "del sentiment140"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-charge",
   "metadata": {},
   "source": [
    "# Amazon Product Reviews (5-core)\n",
    "\n",
    "https://nijianmo.github.io/amazon/index.html\n",
    "\n",
    "* We can use the product rating as a proxy for the sentiment of the review\n",
    "* Since this dataset is much large than I need/could possibly use, I was pretty strict with cleaning\n",
    "   * Must be verified purchases\n",
    "   * Removed duplicates on reviewerID and productID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def clean(df):\n",
    "    df = df[df['verified']]\n",
    "    df = df.drop_duplicates(subset=['reviewerID', 'asin'])\n",
    "    df['date'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
    "    df.dropna(subset=['reviewerID', 'asin', 'overall', 'reviewText'], inplace=True)\n",
    "    return df[['date', 'reviewerID', 'asin', 'reviewerName', 'overall', 'reviewText', 'summary', 'vote']]\n",
    "\n",
    "\n",
    "UNIQUE_COLS = ('reviewerID', 'asin')\n",
    "\n",
    "def getDF(path):\n",
    "    unique = set()\n",
    "    data = []\n",
    "    for i, d in tqdm(enumerate(parse(path))):\n",
    "        try:  # Drop uniques as data is read in to reduce memory usage\n",
    "            key = tuple(d[col] for col in UNIQUE_COLS)\n",
    "            if key in unique:\n",
    "                continue\n",
    "            else:\n",
    "                unique.add(key)\n",
    "                data.append(d)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    print('data loaded')\n",
    "    df = pd.DataFrame(data)\n",
    "    print('dataframe created\\ncleaning...')\n",
    "    return clean(df)\n",
    "\n",
    "def get_df_chunks(path, limit=3000000):\n",
    "    \"\"\"Helper function to chunk gzip into multiple csv's because I don't have enough RAM\"\"\"\n",
    "    lim = limit\n",
    "    data = []\n",
    "    path_template = path[:-8]\n",
    "    chunk = 1\n",
    "    for i, d in tqdm(enumerate(parse(path), 1)):\n",
    "        data.append(d)\n",
    "        if i >= limit:\n",
    "            print(f'saving chunk {chunk}')\n",
    "            df = clean(pd.DataFrame(data))\n",
    "            df.to_csv(f\"{path_template}_{chunk}.csv\", index=False)\n",
    "            df = None\n",
    "            data = []\n",
    "            limit += lim\n",
    "            chunk += 1\n",
    "    df = clean(pd.DataFrame(data))\n",
    "    df.to_csv(f\"{path_template}_{chunk}.csv\", index=False)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-payday",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in list(os.listdir('../data/amazon')):\n",
    "    if file.endswith('.json.gz'):\n",
    "        print(file)\n",
    "        df = getDF(os.path.join('../data/amazon', file))\n",
    "        print(df.shape)\n",
    "        df.to_csv(os.path.join('../data/amazon', f\"{file.split('.')[0]}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_chunks('../data/amazon/books.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/amazon/books_*.csv')\n",
    "df = pd.concat([pd.read_csv(file) for file in  tqdm(files)])\n",
    "df = df.drop_duplicates(subset=['reviewerID', 'asin'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[['date', 'reviewerID', 'asin', 'reviewerName', 'overall', 'reviewText', 'summary', 'vote']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/amazon/books.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-singer",
   "metadata": {},
   "source": [
    "# IMDB Movie Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('../data/imdb/imdb.csv')\n",
    "imdb['sentiment'] = (imdb['sentiment'] == 'positive').astype(np.int8)\n",
    "print(imdb.info())\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-pencil",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "experiment = imdb.copy()\n",
    "experiment['len'] = experiment['review'].str.len()\n",
    "experiment = experiment.groupby('len')\n",
    "\n",
    "text_reg = experiment['sentiment'].mean()\n",
    "text_count = experiment.apply(len)\n",
    "\n",
    "sns.regplot(x=text_reg.index, y=text_reg, ax=ax1)\n",
    "ax1.set_title('Linear Regression of Text Length against Sentiment')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "ax2.plot(text_count.index, text_count)\n",
    "ax2.set_title('Frequency of samples at each length')\n",
    "ax2.set_xlabel('Text Length')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "del experiment, text_reg, text_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb.to_csv('../data/imdb/imdb_binary.csv', index=False)\n",
    "del imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-diamond",
   "metadata": {},
   "source": [
    "# Twitter US Airline Sentiment\n",
    "\n",
    "https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n",
    "Not good for this project, only gives text (10 preset choices) for negative responses. Would lead to sampling bias and the preselected responses are too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/twitter_airlines/tweets.csv')\n",
    "print(tweets.info())\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['negativereason'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-century",
   "metadata": {},
   "source": [
    "# Bag of Words Meets Popcorn Dataset\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "popcorn_train = pd.read_csv('../data/popcorn/train.csv')\n",
    "popcorn_test = pd.read_csv('../data/popcorn/test.csv')\n",
    "print(popcorn_train.info())\n",
    "print(popcorn_test.info())\n",
    "popcorn_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "experiment = popcorn_train.copy()\n",
    "experiment['len'] = experiment['review'].str.len()\n",
    "experiment = experiment.groupby('len')\n",
    "\n",
    "text_reg = experiment['sentiment'].mean()\n",
    "text_count = experiment.apply(len)\n",
    "\n",
    "sns.regplot(x=text_reg.index, y=text_reg, ax=ax1)\n",
    "ax1.set_title('Linear Regression of Text Length against Sentiment')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "ax2.plot(text_count.index, text_count)\n",
    "ax2.set_title('Frequency of samples at each length')\n",
    "ax2.set_xlabel('Text Length')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "del experiment, text_reg, text_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('../data/imdb/imdb_binary.csv')\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "popcorn_train['review'].isin(imdb['review']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-scanner",
   "metadata": {},
   "source": [
    "There is huge **exact** overlap with the imdb dataset so I'm going to assume most if not all of this is duplicated from that dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-queensland",
   "metadata": {},
   "source": [
    "# OpinRank hotel and car reviews\n",
    "\n",
    "http://kavita-ganesan.com/entity-ranking-data/#.W4jjE5MzbUJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_csvs(path):\n",
    "    for a, b, c, in os.walk(path):\n",
    "        for i in c:\n",
    "            if i.endswith('.csv'):\n",
    "                yield os.path.join(a, i)\n",
    "\n",
    "for i in _get_csvs('../data/opinrank'):\n",
    "    print(pd.read_csv(i).head())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-interference",
   "metadata": {},
   "source": [
    "**Also unusable for this project since it doesn't really contain text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-intention",
   "metadata": {},
   "source": [
    "# Yelp NYC Dataset\n",
    "\n",
    "359k yelp reviews for restuarants in NYC\n",
    "\n",
    "http://odds.cs.stonybrook.edu/yelpnyc-dataset/\n",
    "\n",
    "https://www.kaggle.com/ahtxham/yelpnyc-labelled-dataset-from-shebuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('../data/yelpnyc/yelp.csv')\n",
    "ratings = pd.read_csv('../data/yelpnyc/yelp_meta.csv')\n",
    "print(reviews.info())\n",
    "print(ratings.info())\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpnyc = reviews.merge(ratings, how='inner', on=['Review_id', 'Product_id'])\n",
    "yelpnyc.drop('Review_Date_y', axis=1, inplace=True)\n",
    "yelpnyc.columns = ['review_id', 'product_id', 'date', 'text', 'rating', 'label']\n",
    "yelpnyc['text'] = yelpnyc['text'].str.replace(\"Â\\xa0\", '')\n",
    "yelpnyc['date'] = pd.to_datetime(yelpnyc['date'])\n",
    "yelpnyc['label'] = ((yelpnyc['label'] + 1) / 2).astype(np.int8)  # [-1, 1] label to [0, 1]\n",
    "yelpnyc.info()\n",
    "yelpnyc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "experiment = yelpnyc.copy()\n",
    "experiment['day'] = experiment['date'].dt.date\n",
    "experiment['len'] = experiment['text'].str.len()\n",
    "grouped = experiment.groupby('day')\n",
    "\n",
    "experiment['len'].plot.hist(bins=30, ax=ax1)\n",
    "grouped.apply(len).plot(ax=ax2)\n",
    "grouped['label'].mean().plot(ax=ax4)\n",
    "text_reg = experiment.groupby('len')['label'].mean()\n",
    "sns.regplot(x=text_reg.index, y=text_reg, ax=ax3)\n",
    "\n",
    "ax1.set_title('Histogram of Text Lengths')\n",
    "ax1.set_xlabel('Text Length')\n",
    "ax2.set_title('Number of Reviews per Day')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax3.set_title('Linear Regression of Text Length against Sentiment')\n",
    "ax3.set_xlabel('Text Length')\n",
    "ax3.set_ylabel('Sentiment')\n",
    "ax4.set_title('Average Sentiment per Day')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Sentiment')\n",
    "plt.subplots_adjust(top=1.2, right=1.5)\n",
    "plt.show()\n",
    "\n",
    "del experiment, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpnyc.to_csv('../data/yelpnyc/yelpnyc_binary.csv', index=False)\n",
    "del yelpnyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experienced-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "optional-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size (bytes)</th>\n",
       "      <th>Date Modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RC_2018-01-01.xz</td>\n",
       "      <td>LZMA2 Compressed Reddit Comments (JSON objects)</td>\n",
       "      <td>201491588</td>\n",
       "      <td>Feb 13 2018 1:52 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RC_2018-01-02.xz</td>\n",
       "      <td>LZMA2 Compressed Reddit Comments (JSON objects)</td>\n",
       "      <td>257462668</td>\n",
       "      <td>Feb 13 2018 1:52 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RC_2018-01-03.xz</td>\n",
       "      <td>LZMA2 Compressed Reddit Comments (JSON objects)</td>\n",
       "      <td>272128832</td>\n",
       "      <td>Feb 13 2018 1:52 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RC_2018-01-04.xz</td>\n",
       "      <td>LZMA2 Compressed Reddit Comments (JSON objects)</td>\n",
       "      <td>263591508</td>\n",
       "      <td>Feb 13 2018 1:53 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RC_2018-01-05.xz</td>\n",
       "      <td>LZMA2 Compressed Reddit Comments (JSON objects)</td>\n",
       "      <td>276650320</td>\n",
       "      <td>Feb 13 2018 1:53 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>RC_2020-04-16.gz</td>\n",
       "      <td>Reddit Comments (JSON objects)</td>\n",
       "      <td>1192552544</td>\n",
       "      <td>Sep 1 2020 10:08 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>RC_2020-04-17.gz</td>\n",
       "      <td>Reddit Comments (JSON objects)</td>\n",
       "      <td>1163581603</td>\n",
       "      <td>Sep 1 2020 10:09 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>RC_2020-04-18.gz</td>\n",
       "      <td>Reddit Comments (JSON objects)</td>\n",
       "      <td>1098207690</td>\n",
       "      <td>Sep 1 2020 10:09 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>par_recovery</td>\n",
       "      <td>&lt;Directory&gt; File</td>\n",
       "      <td>&lt;Directory&gt;</td>\n",
       "      <td>Mar 17 2018 8:06 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>sha256sum.txt</td>\n",
       "      <td>Text File</td>\n",
       "      <td>4898</td>\n",
       "      <td>Mar 11 2018 8:22 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Filename                                             Type  \\\n",
       "0    RC_2018-01-01.xz  LZMA2 Compressed Reddit Comments (JSON objects)   \n",
       "1    RC_2018-01-02.xz  LZMA2 Compressed Reddit Comments (JSON objects)   \n",
       "2    RC_2018-01-03.xz  LZMA2 Compressed Reddit Comments (JSON objects)   \n",
       "3    RC_2018-01-04.xz  LZMA2 Compressed Reddit Comments (JSON objects)   \n",
       "4    RC_2018-01-05.xz  LZMA2 Compressed Reddit Comments (JSON objects)   \n",
       "..                ...                                              ...   \n",
       "193  RC_2020-04-16.gz                   Reddit Comments (JSON objects)   \n",
       "194  RC_2020-04-17.gz                   Reddit Comments (JSON objects)   \n",
       "195  RC_2020-04-18.gz                   Reddit Comments (JSON objects)   \n",
       "196      par_recovery                                 <Directory> File   \n",
       "197     sha256sum.txt                                        Text File   \n",
       "\n",
       "    Size (bytes)        Date Modified  \n",
       "0      201491588  Feb 13 2018 1:52 AM  \n",
       "1      257462668  Feb 13 2018 1:52 AM  \n",
       "2      272128832  Feb 13 2018 1:52 AM  \n",
       "3      263591508  Feb 13 2018 1:53 AM  \n",
       "4      276650320  Feb 13 2018 1:53 AM  \n",
       "..           ...                  ...  \n",
       "193   1192552544  Sep 1 2020 10:08 AM  \n",
       "194   1163581603  Sep 1 2020 10:09 AM  \n",
       "195   1098207690  Sep 1 2020 10:09 AM  \n",
       "196  <Directory>  Mar 17 2018 8:06 PM  \n",
       "197         4898  Mar 11 2018 8:22 PM  \n",
       "\n",
       "[198 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = pd.read_html('https://files.pushshift.io/reddit/comments/daily/')[0]\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beneficial-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import lzma\n",
    "import gzip\n",
    "import zstandard\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "meaningful-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['id', 'created_utc', 'retrieved_on', 'subreddit', 'author', 'body']\n",
    "\n",
    "def valid_line(line, subreddits):\n",
    "    sub_check = 'subreddit' in line and line['subreddit'] in subreddits\n",
    "    body_check = 'body' in line and line['body'] != '[deleted]' and line['body'] != '[removed]'\n",
    "    has_elem = 'created_utc' in line and 'id' in line and 'retrieved_on' in line and 'author' in line\n",
    "    return sub_check and body_check and has_elem\n",
    "\n",
    "def clean_reddit(path, subreddits=['investing', 'stocks', 'wallstreetbets'], cols=COLS):\n",
    "    data = []\n",
    "    out_path = '/'.join(path.split('/')[:-1]) + '/' + path.split('/')[-1].split('.')[0].lower() + '.csv'\n",
    "    print(f\"Loading file: {path}\")\n",
    "    with gzip.open(path) as f:\n",
    "        for line in tqdm(f):\n",
    "            d = json.loads(line)\n",
    "            if valid_line(d, subreddits):\n",
    "                data.append({k: d[k] for k in cols})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop_duplicates(subset=['id', 'subreddit'], inplace=True)\n",
    "    if df.shape[0] == 0:\n",
    "        print(f'No valid rows found, not saving file: {path}')\n",
    "    else:\n",
    "        print(df.shape)\n",
    "        print(f\"saving to {out_path}\")\n",
    "        df.to_csv(out_path)\n",
    "    return df\n",
    "\n",
    "def clean_reddit_zst(path, subreddits=['investing', 'stocks', 'wallstreetbets'], cols=COLS):\n",
    "    data = []\n",
    "    out_path = '/'.join(path.split('/')[:-1]) + '/' + path.split('/')[-1].split('.')[0].lower() + '.csv'\n",
    "    print(f\"Loading file: {path}\")\n",
    "    with open(path, 'rb') as f:\n",
    "        dctx = zstandard.ZstdDecompressor()\n",
    "        stream_reader = dctx.stream_reader(f)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        for line in tqdm(text_stream):\n",
    "            d = json.loads(line)\n",
    "            if valid_line(d, subreddits):\n",
    "                data.append({k: d[k] for k in cols})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop_duplicates(subset=['id', 'subreddit'], inplace=True)\n",
    "    if df.shape[0] == 0:\n",
    "        print(f'No valid rows found, not saving file: {path}')\n",
    "    else:\n",
    "        print(df.shape)\n",
    "        print(f\"saving to {out_path}\")\n",
    "        df.to_csv(out_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hidden-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_dir = '../scraped_data/pushshift/'\n",
    "\n",
    "for file in foo['Filename'].sort_values().values[87:-2]:\n",
    "    if not os.path.exists(os.path.join(push_dir, file)):\n",
    "        file_url = \"https://files.pushshift.io/reddit/comments/daily/\" + file\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        print(f\"File downloaded {wget.download(file_url, out=push_dir)}\")\n",
    "    print(\"Cleaning file...\")\n",
    "    print(f\"File cleaned:\\n{clean_reddit(os.path.join(push_dir, file)).info()}\")\n",
    "    file_out = os.path.join(push_dir, file)\n",
    "    print(f\"Deleting file: {file_out}\")\n",
    "    os.remove(file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "short-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/.cache/pypoetry/virtualenvs/nlp-research-1iGXy4V8-py3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14666703 entries, 0 to 17553\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   Unnamed: 0    object \n",
      " 1   id            object \n",
      " 2   created_utc   float64\n",
      " 3   retrieved_on  float64\n",
      " 4   subreddit     object \n",
      " 5   author        object \n",
      " 6   body          object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 895.2+ MB\n"
     ]
    }
   ],
   "source": [
    "push_dir = '../scraped_data/pushshift/'\n",
    "merged = pd.concat([pd.read_csv(os.path.join(push_dir, i)) for i in os.listdir(push_dir) if i.endswith('.csv')])\n",
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "framed-terrorism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14666328 entries, 0 to 102545\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   id            object        \n",
      " 1   created_utc   datetime64[ns]\n",
      " 2   retrieved_on  datetime64[ns]\n",
      " 3   subreddit     object        \n",
      " 4   author        object        \n",
      " 5   body          object        \n",
      "dtypes: datetime64[ns](2), object(4)\n",
      "memory usage: 783.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6ulr69</td>\n",
       "      <td>2012-11-01 00:06:35</td>\n",
       "      <td>2015-04-29 06:24:26</td>\n",
       "      <td>investing</td>\n",
       "      <td>clituna</td>\n",
       "      <td>Check Craigslist. That's where we sell a lot o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6uls3f</td>\n",
       "      <td>2012-11-01 00:08:29</td>\n",
       "      <td>2015-04-29 06:24:38</td>\n",
       "      <td>investing</td>\n",
       "      <td>Is_this_thing_on</td>\n",
       "      <td>I couldn't tell you. Their numbers look pretty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c6ulsz0</td>\n",
       "      <td>2012-11-01 00:10:25</td>\n",
       "      <td>2015-04-29 06:24:48</td>\n",
       "      <td>investing</td>\n",
       "      <td>kage860</td>\n",
       "      <td>Short : AMZN\\n\\nRational: Rich Valuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c6uluh1</td>\n",
       "      <td>2012-11-01 00:13:35</td>\n",
       "      <td>2015-04-29 06:25:08</td>\n",
       "      <td>investing</td>\n",
       "      <td>yobria</td>\n",
       "      <td>If the market thought X was a shitty investmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c6ulw23</td>\n",
       "      <td>2012-11-01 00:17:01</td>\n",
       "      <td>2015-06-30 14:09:56</td>\n",
       "      <td>investing</td>\n",
       "      <td>Is_this_thing_on</td>\n",
       "      <td>Don't totally discount game publishers though....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102543</th>\n",
       "      <td>fnu4ipt</td>\n",
       "      <td>2020-04-18 23:59:58</td>\n",
       "      <td>2020-07-05 00:55:14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>ZAYN91</td>\n",
       "      <td>Calls on WSB.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102541</th>\n",
       "      <td>fnu4ip7</td>\n",
       "      <td>2020-04-18 23:59:58</td>\n",
       "      <td>2020-07-05 00:55:14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>twotomatoes</td>\n",
       "      <td>HELLLLLLPPPPP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102544</th>\n",
       "      <td>fnu4iq0</td>\n",
       "      <td>2020-04-18 23:59:58</td>\n",
       "      <td>2020-07-05 00:55:14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Andrew_the_giant</td>\n",
       "      <td>Aw hell I'm in. What's to lose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102546</th>\n",
       "      <td>fnu4isv</td>\n",
       "      <td>2020-04-18 23:59:59</td>\n",
       "      <td>2020-07-05 00:55:15</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>ZAYN91</td>\n",
       "      <td>Calls on WSB.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102545</th>\n",
       "      <td>fnu4irq</td>\n",
       "      <td>2020-04-18 23:59:59</td>\n",
       "      <td>2020-07-05 00:55:15</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>thenamesake11</td>\n",
       "      <td>Lets do it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14666328 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id         created_utc        retrieved_on       subreddit  \\\n",
       "0       c6ulr69 2012-11-01 00:06:35 2015-04-29 06:24:26       investing   \n",
       "1       c6uls3f 2012-11-01 00:08:29 2015-04-29 06:24:38       investing   \n",
       "2       c6ulsz0 2012-11-01 00:10:25 2015-04-29 06:24:48       investing   \n",
       "3       c6uluh1 2012-11-01 00:13:35 2015-04-29 06:25:08       investing   \n",
       "4       c6ulw23 2012-11-01 00:17:01 2015-06-30 14:09:56       investing   \n",
       "...         ...                 ...                 ...             ...   \n",
       "102543  fnu4ipt 2020-04-18 23:59:58 2020-07-05 00:55:14  wallstreetbets   \n",
       "102541  fnu4ip7 2020-04-18 23:59:58 2020-07-05 00:55:14  wallstreetbets   \n",
       "102544  fnu4iq0 2020-04-18 23:59:58 2020-07-05 00:55:14  wallstreetbets   \n",
       "102546  fnu4isv 2020-04-18 23:59:59 2020-07-05 00:55:15  wallstreetbets   \n",
       "102545  fnu4irq 2020-04-18 23:59:59 2020-07-05 00:55:15  wallstreetbets   \n",
       "\n",
       "                  author                                               body  \n",
       "0                clituna  Check Craigslist. That's where we sell a lot o...  \n",
       "1       Is_this_thing_on  I couldn't tell you. Their numbers look pretty...  \n",
       "2                kage860           Short : AMZN\\n\\nRational: Rich Valuation  \n",
       "3                 yobria  If the market thought X was a shitty investmen...  \n",
       "4       Is_this_thing_on  Don't totally discount game publishers though....  \n",
       "...                  ...                                                ...  \n",
       "102543            ZAYN91                                      Calls on WSB.  \n",
       "102541       twotomatoes                                      HELLLLLLPPPPP  \n",
       "102544  Andrew_the_giant                     Aw hell I'm in. What's to lose  \n",
       "102546            ZAYN91                                      Calls on WSB.  \n",
       "102545     thenamesake11                                         Lets do it  \n",
       "\n",
       "[14666328 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.dropna(inplace=True)\n",
    "merged.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "merged['created_utc'] = pd.to_datetime(merged['created_utc'], unit='s')\n",
    "merged['retrieved_on'] = pd.to_datetime(merged['retrieved_on'], unit='s')\n",
    "merged.sort_values('created_utc', inplace=True)\n",
    "print(merged.info())\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollywood-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../scraped_data/pushshift/cleaned/comments_2012_11_2020_4_18.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
